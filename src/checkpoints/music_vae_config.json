[
  {
    "name": "drums_2bar_lokl_small",
    "model": "MusicVAE",
    "description": "A 2-bar, 9-class onehot drum model with a strong prior (low KL divergence), which is better for sampling. Less accurate, but smaller in size than full model.",
    "checkpoint": "drums_2bar_lokl_small",
    "size_mb": 18.5
  },
  {
    "name": "drums_2bar_hikl_small",
    "model": "MusicVAE",
    "description": "A 2-bar, 9-class onehot drum model with a weak prior (higher KL divergence), which is better for reconstructions and interpolations. Less accurate, but smaller in size than full model.",
    "checkpoint": "drums_2bar_hikl_small",
    "size_mb": 18.5
  },
  {
    "name": "drums_2bar_nade_9_q2",
    "model": "MusicVAE",
    "description": "A 2-bar, 9-class multilabel drum model with a NADE decoder. Quantized to 2-byte weights.",
    "checkpoint": "drums_2bar_nade_9_q2",
    "size_mb": 27.6
  },
  {
    "name": "drums_4bar_med_q2",
    "model": "MusicVAE",
    "description": "A medium-sized 2-bar, 9-class onehot drum model with a weak prior (higher KL divergence), which is better for reconstructions and interpolations. Quantized to 2-byte weights.",
    "checkpoint": "drums_4bar_med_q2",
    "size_mb": 68.2
  },
  {
    "name": "drums_4bar_med_lokl_q2",
    "model": "MusicVAE",
    "description": "A medium-sized 2-bar, 9-class onehot drum model with a strong prior (lower KL divergence), which is better for sampling. Quantized to 2-byte weights.",
    "checkpoint": "drums_4bar_med_lokl_q2",
    "size_mb": 68.2
  },
  {
    "name": "mel_2bar_small",
    "model": "MusicVAE",
    "description": "A 2-bar, 90-class onehot melody model. Less accurate, but smaller in size than full model.",
    "checkpoint": "mel_2bar_small",
    "size_mb": 17.7
  },
  {
    "name": "mel_4bar_med_q2",
    "model": "MusicVAE",
    "description": "A medium-sized 4-bar, 90-class onehot melody model. Quantized to 2-byte weights.",
    "checkpoint": "mel_4bar_med_q2",
    "size_mb": 65.4
  },
  {
    "name": "mel_4bar_med_lokl_q2",
    "model": "MusicVAE",
    "description": "A medium-sized 4-bar, 90-class onehot melody model.  Trained with a strong prior (low KL divergence), which is better for sampling. Quantized to 2-byte weights.",
    "checkpoint": "mel_4bar_med_lokl_q2",
    "size_mb": 65.4
  },
  {
    "name": "mel_4bar_small_q2",
    "model": "MusicVAE",
    "description": "A 4-bar, 90-class onehot melody model. Less accurate, but smaller in size than full model. Quantized to 2-byte weights.",
    "checkpoint": "mel_4bar_small_q2",
    "size_mb": 26.5
  },
  {
    "name": "mel_chords",
    "model": "MusicVAE",
    "description": "A 2-bar, 90-class onehot melody model with chord conditioning. Quantized to 2-byte weights.",
    "checkpoint": "mel_chords",
    "size_mb": 17.6
  },
  {
    "name": "mel_16bar_small_q2",
    "model": "MusicVAE",
    "description": "A 16-bar, 90-class onehot melody model with a 16-step conductor level. Less accurate, but smaller in size than full model. Quantized to 2-byte weights.",
    "checkpoint": "mel_16bar_small_q2",
    "size_mb": 23.5
  },
  {
    "name": "trio_4bar_lokl_small_q1",
    "model": "MusicVAE",
    "description": "A 4-bar, 'trio' model for melody, bass, and drums, with a 4-step conductor level. Trained with a strong prior (low KL divergence), which is better for sampling. Less accurate, but smaller in size than full model. Quantized to 1-byte weights.",
    "checkpoint": "trio_4bar",
    "size_mb": 17.6
  },
  {
    "name": "multitrack",
    "model": "MusicVAE",
    "description": "A 1-bar multitrack model, trained with 64 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack",
    "size_mb": 26.4
  },
  {
    "name": "multitrack_fb256",
    "model": "MusicVAE",
    "description": "A 1-bar multitrack model, trained with 256 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_fb256",
    "size_mb": 95.9
  },
  {
    "name": "multitrack_med",
    "model": "MusicVAE",
    "description": "A larger 1-bar multitrack model, trained with 64 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_med",
    "size_mb": 95.9
  },
  {
    "name": "multitrack_med_fb256",
    "model": "MusicVAE",
    "description": "A larger 1-bar multitrack model, trained with 256 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_med_fb256",
    "size_mb": 95.9
  },
  {
    "name": "multitrack_chords",
    "model": "MusicVAE",
    "description": "A 1-bar chord-conditioned multitrack model, trained with 64 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_chords",
    "size_mb": 26.9
  },
  {
    "name": "multitrack_med_chords",
    "model": "MusicVAE",
    "description": "A larger 1-bar chord-conditioned multitrack model, trained with 64 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_med_chords",
    "size_mb": 96.9
  },
  {
    "name": "multitrack_med_chords_fb256",
    "model": "MusicVAE",
    "description": "A larger 1-bar chord-conditioned multitrack model, trained with 256 free bits. Quantized to 1-byte weights.",
    "checkpoint": "multitrack_med_chords_fb256",
    "size_mb": 96.9
  },
  {
    "name": "groovae_2bar_humanize",
    "model": "MusicVAE",
    "description": "A 2-bar GrooVAE model that converts a quantized, constant-velocity drum pattern into a 'humanized' groove.",
    "checkpoint": "groovae_2bar_humanize",
    "size_mb": 15.8
  },
  {
    "name": "groovae_2bar_tap2drum",
    "model": "MusicVAE",
    "description": "A 2-bar GrooVAE model that converts a constant-velocity single-drum 'tap' pattern into a groove.",
    "checkpoint": "groovae_2bar_tap2drum",
    "size_mb": 15.8
  },
  {
    "name": "groovae_4bar",
    "model": "MusicVAE",
    "description": "A 4-bar GrooVAE autoencoder.",
    "checkpoint": "groovae_4bar",
    "size_mb": 15.8
  }
]
